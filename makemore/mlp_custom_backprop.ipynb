{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "print(chars)\n",
    "stoi = {s: i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s,i in stoi.items()}\n",
    "print(itos)\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.Size([182625, 3]) torch.int64 torch.Size([182625])\n",
      "torch.int64 torch.Size([22655, 3]) torch.int64 torch.Size([22655])\n",
      "torch.int64 torch.Size([22866, 3]) torch.int64 torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        # print(w)\n",
    "        context = [0]*block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X).to(device)\n",
    "    Y = torch.tensor(Y).to(device)\n",
    "    print(X.dtype, X.shape, Y.dtype, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])        # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])    # 10%\n",
    "Xte, Yte = build_dataset(words[n2:])        # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size,n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "dlogprops       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dprobs          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcountsuminv    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcountsum       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dnormlogits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogitsmax      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhpreact        | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "dbngain         | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-10\n",
      "dbnraw          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "dbnbias         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dbnstdinv       | exact: False | approximate: True  | maxdiff: 3.4924596548080444e-10\n",
      "dbnvar          | exact: False | approximate: True  | maxdiff: 8.731149137020111e-11\n",
      "dbndiff2        | exact: False | approximate: True  | maxdiff: 3.637978807091713e-12\n",
      "dbndiff         | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "dbnmeani        | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dhprebn         | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-10\n",
      "dembcat         | exact: False | approximate: True  | maxdiff: 3.4924596548080444e-10\n",
      "dW1             | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
      "db1             | exact: False | approximate: True  | maxdiff: 4.3655745685100555e-10\n",
      "demb            | exact: False | approximate: True  | maxdiff: 3.4924596548080444e-10\n",
      "dC              | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb = Xtr[ix]\n",
    "Yb = Ytr[ix]\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    p.grad = None\n",
    "\n",
    "# forward pass\n",
    "emb = C[Xb] # embed the characters into vectors - (batch_size x block_size x n_embed)\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors - (batch_size x (block_size x n_embed))\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation - (batch_size x n_hidden)\n",
    "bnmeani = 1.0/batch_size * hprebn.sum(dim=0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff ** 2\n",
    "bnvar = 1.0/(batch_size-1) * bndiff2.sum(dim=0, keepdim=True)\n",
    "bnstdinv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnstdinv\n",
    "hpreact = bngain * bnraw + bnbias # (batch_size x n_hidden)\n",
    "h = torch.tanh(hpreact) # hidden layer - (batch_size x n_hidden)\n",
    "logits = h @ W2 + b2 # output layer - (batch_size x vocab_size)\n",
    "# logits = torch.rand((batch_size, 27), requires_grad=True)\n",
    "logitsmax = logits.max(dim=1, keepdim=True).values # (batch_size x vocab_size)\n",
    "normlogits = logits - logitsmax # (batch_size x vocab_size)\n",
    "counts = normlogits.exp() # (batch_size x vocab_size)\n",
    "countsum = counts.sum(dim=1, keepdim=True) # (batch_size x 1)\n",
    "countsuminv = countsum ** -1 # (batch_size x 1)\n",
    "probs = counts * countsuminv # (batch_size x vocab_size)\n",
    "logprobs = probs.log() # (batch_size x vocab_size)\n",
    "loss = -logprobs[torch.arange(batch_size), Yb].mean() # 1\n",
    "\n",
    "for t in [emb, C, hprebn, embcat, W1, b1, bnmeani, bndiff, bndiff2, bnvar, bndiff, bnstdinv, bngain, bnraw, bnbias, hpreact, h, W2, b2, logits, logitsmax, normlogits, counts, countsum, countsuminv, probs, logprobs, loss]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "# backward pass\n",
    "dlogprops = torch.zeros(logprobs.shape)\n",
    "dlogprops[torch.arange(batch_size), Yb] = -1.0/batch_size\n",
    "dprobs = (1.0 / probs) * dlogprops\n",
    "dcountsuminv = (counts * dprobs).sum(dim=1, keepdim=True)\n",
    "dcountsum = (-countsum**-2) * dcountsuminv\n",
    "dcounts = torch.ones_like(counts) * dcountsum\n",
    "dcounts += countsuminv * dprobs\n",
    "dnormlogits = counts * dcounts\n",
    "dlogitsmax = -dnormlogits.sum(dim=1, keepdim=True) # for: -logitsmax in: normlogits = logits - logitsmax\n",
    "dlogits = F.one_hot(logits.argmax(dim=1), num_classes=logits.shape[1]) * dlogitsmax\n",
    "dlogits += dnormlogits.clone()\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(dim=0, keepdim=True)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "# dhpreact = (1.0/(torch.arccosh(hpreact)**2)) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(dim=0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(dim=0, keepdim=True)\n",
    "dbnstdinv = (bndiff * dbnraw).sum(dim=0, keepdim=True)\n",
    "dbnvar = -0.5*(bnvar + 1e-5)**-1.5 * dbnstdinv\n",
    "dbndiff2 = 1.0/(batch_size-1) * torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff = bnstdinv * dbnraw\n",
    "dbndiff += 2*bndiff * dbndiff2\n",
    "dbnmeani = -dbndiff.sum(dim=0, keepdim=True)\n",
    "dhprebn = dbndiff + 1.0/batch_size * torch.ones_like(hprebn) * dbnmeani\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(dim=0, keepdim=True)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        k = Xb[i,j]\n",
    "        dC[k] += demb[i, j]\n",
    "\n",
    "cmp('dlogprops', dlogprops, logprobs)\n",
    "cmp('dprobs', dprobs, probs)\n",
    "cmp('dcountsuminv', dcountsuminv, countsuminv)\n",
    "cmp('dcountsum', dcountsum, countsum)\n",
    "cmp('dcounts', dcounts, counts)\n",
    "cmp('dnormlogits', dnormlogits, normlogits)\n",
    "cmp('dlogitsmax', dlogitsmax, logitsmax)\n",
    "cmp('dlogits', dlogits, logits)\n",
    "cmp('dh', dh, h)\n",
    "cmp('dW2', dW2, W2)\n",
    "cmp('db2', db2, b2)\n",
    "cmp('dhpreact', dhpreact, hpreact)\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbnbias', dbnbias, bnbias)\n",
    "cmp('dbnstdinv', dbnstdinv, bnstdinv)\n",
    "cmp('dbnvar', dbnvar, bnvar)\n",
    "cmp('dbndiff2', dbndiff2, bndiff2)\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "cmp('demb', demb, emb)\n",
    "cmp('dC', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3146891593933105 3.3146886825561523 diff: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss.item(), loss_fast.item(), 'diff:', (loss - loss_fast).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 27]) torch.Size([32, 27])\n",
      "diff: 6.6356733441352844e-09\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape, dlogits.shape)\n",
    "dlogits_fast = F.softmax(logits, dim=1)\n",
    "dlogits_fast[torch.arange(batch_size), Yb] -= 1\n",
    "dlogits_fast /= batch_size\n",
    "print('diff:', (dlogits - dlogits_fast).max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0011, -0.0298,  0.0011,  0.0012,  0.0012,  0.0011,  0.0011,  0.0011,\n",
       "         0.0012,  0.0012,  0.0013,  0.0010,  0.0011,  0.0010,  0.0011,  0.0011,\n",
       "         0.0012,  0.0010,  0.0009,  0.0011,  0.0014,  0.0013,  0.0013,  0.0011,\n",
       "         0.0010,  0.0013,  0.0013], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')\n",
    "dlogits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bngain diff: 6.05359673500061e-09\n",
      "bnbias diff: 0.0\n",
      "torch.Size([32, 200]) torch.Size([1, 200]) torch.Size([32, 200])\n"
     ]
    }
   ],
   "source": [
    "dbngain_fast = (((hprebn - hprebn.mean(dim=0, keepdim=True)) / hprebn.std(dim=0, keepdim=True)) * dhpreact).sum(dim=0, keepdim=True)\n",
    "dbnbias_fast = dhpreact.sum(dim=0, keepdim=True)\n",
    "# dhprebn_fast += bngain * (hprebn - bnmeani) * -bnstdinv**2 * 2/(batch_size-1) * (hprebn - bnmeani).sum(dim=0, keepdim=True) * (1.0 - 1.0/batch_size) * dhpreact\n",
    "print('bngain diff:', (dbngain - dbngain_fast).max().item())\n",
    "print('bnbias diff:', (dbnbias - dbnbias_fast).max().item())\n",
    "print(dhprebn.shape, bngain.shape, hprebn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200]) torch.Size([1, 200])\n",
      "bnvar diff: 7.043126970529556e-09\n"
     ]
    }
   ],
   "source": [
    "dbnvar_fast = (dhpreact * (-0.5*bngain * bndiff * bnvar**-1.5)).sum(dim=0, keepdim=True)\n",
    "print(dbnvar_fast.shape, dbnvar.shape)\n",
    "print('bnvar diff:', ((dbnvar_fast - dbnvar).max().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnmean diff: 2.60770320892334e-08\n"
     ]
    }
   ],
   "source": [
    "dbnmean_fast = (dhpreact * -bngain * bnvar**-0.5).sum(dim=0, keepdim=True)\n",
    "print('bnmean diff:', (dbnmean_fast - dbnmeani).max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200]) torch.Size([32, 200]) torch.Size([1, 200]) torch.Size([32, 200])\n",
      "hprebn diff: 5.820766091346741e-09\n"
     ]
    }
   ],
   "source": [
    "temp1 = dhpreact * bngain * bnvar**-0.5\n",
    "temp2 = dbnmean_fast * 1/batch_size * torch.ones_like(hprebn)\n",
    "temp3 = dbnvar_fast * 2/(batch_size-1) * bndiff\n",
    "dhprebn_fast = temp1 + temp2 + temp3\n",
    "print(dhprebn.shape, dhprebn_fast.shape, bngain.shape, hprebn.shape)\n",
    "print('hprebn diff:', (dhprebn - dhprebn_fast).max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "dlogits         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dh              | exact: False | approximate: True  | maxdiff: 1.7462298274040222e-10\n",
      "dW2             | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "db2             | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "dhpreact        | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-10\n",
      "dbngain         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-10\n",
      "dbnraw          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "dbnbias         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dhprebn         | exact: False | approximate: True  | maxdiff: 1.4551915228366852e-10\n",
      "dembcat         | exact: False | approximate: True  | maxdiff: 5.820766091346741e-10\n",
      "dW1             | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "db1             | exact: False | approximate: True  | maxdiff: 5.456968210637569e-10\n",
      "demb            | exact: False | approximate: True  | maxdiff: 5.820766091346741e-10\n",
      "dC              | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb = Xtr[ix]\n",
    "Yb = Ytr[ix]\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True    \n",
    "\n",
    "# forward pass\n",
    "emb = C[Xb] # embed the characters into vectors - (batch_size x block_size x n_embed)\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors - (batch_size x (block_size x n_embed))\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation - (batch_size x n_hidden)\n",
    "hpreact = bngain * (hprebn - hprebn.mean(dim=0, keepdim=True))/hprebn.std(dim=0, keepdim=True) + bnbias # batch-norm\n",
    "h = torch.tanh(hpreact) # hidden layer - (batch_size x n_hidden)\n",
    "logits = h @ W2 + b2 # output layer - (batch_size x vocab_size)\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "\n",
    "for t in [C, W1, b1, W2, b2, bngain, bnbias, emb, embcat, hprebn, hpreact, h, logits, loss]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "# backward pass\n",
    "dlogits = F.softmax(logits, dim=1)\n",
    "dlogits[torch.arange(batch_size), Yb] -= 1\n",
    "dlogits /= batch_size\n",
    "# output layer\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(dim=0, keepdim=True)\n",
    "# tanh\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "# batch-norm\n",
    "bndiff = hprebn - hprebn.mean(dim=0, keepdim=True)\n",
    "bnvar = hprebn.var(dim=0, keepdim=True)\n",
    "dbngain = ((bndiff / hprebn.std(dim=0, keepdim=True)) * dhpreact).sum(dim=0, keepdim=True)\n",
    "dbnbias = dhpreact.sum(dim=0, keepdim=True)\n",
    "dbnvar = (dhpreact * (-0.5*bngain * bndiff * bnvar**-1.5)).sum(dim=0, keepdim=True)\n",
    "dbnmean = (dhpreact * -bngain * bnvar**-0.5).sum(dim=0, keepdim=True)\n",
    "temp1 = dhpreact * bngain * bnvar**-0.5\n",
    "temp2 = dbnmean * 1/batch_size * torch.ones_like(hprebn)\n",
    "temp3 = dbnvar * 2/(batch_size-1) * bndiff\n",
    "dhprebn = temp1 + temp2 + temp3\n",
    "# hidden layers\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(dim=0, keepdim=True)\n",
    "# input layer\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        k = Xb[i,j]\n",
    "        dC[k] += demb[i, j]\n",
    "\n",
    "\n",
    "cmp('dlogits', dlogits, logits)\n",
    "cmp('dh', dh, h)\n",
    "cmp('dW2', dW2, W2)\n",
    "cmp('db2', db2, b2)\n",
    "cmp('dhpreact', dhpreact, hpreact)\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbnbias', dbnbias, bnbias)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "cmp('demb', demb, emb)\n",
    "cmp('dC', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb = Xtr[ix]\n",
    "Yb = Ytr[ix]\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    p.grad = None\n",
    "\n",
    "# forward pass\n",
    "emb = C[Xb] # embed the characters into vectors - (batch_size x block_size x n_embed)\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors - (batch_size x (block_size x n_embed))\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation - (batch_size x n_hidden)\n",
    "hpreact = bngain * (hprebn - hprebn.mean(dim=0, keepdim=True))/hprebn.std(dim=0, keepdim=True) + bnbias # batch-norm\n",
    "h = torch.tanh(hpreact) # hidden layer - (batch_size x n_hidden)\n",
    "logits = h @ W2 + b2 # output layer - (batch_size x vocab_size)\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "\n",
    "for t in [C, W1, b1, W2, b2, bngain, bnbias, emb, embcat, hprebn, hpreact, h, logits, loss]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "# backward pass\n",
    "dlogits = F.softmax(logits, dim=1)\n",
    "dlogits[torch.arange(batch_size), Yb] -= 1\n",
    "dlogits /= batch_size\n",
    "# output layer\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(dim=0, keepdim=True)\n",
    "# tanh\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "# batch-norm\n",
    "bndiff = hprebn - hprebn.mean(dim=0, keepdim=True)\n",
    "bnvar = hprebn.var(dim=0, keepdim=True)\n",
    "dbngain = ((bndiff / hprebn.std(dim=0, keepdim=True)) * dhpreact).sum(dim=0, keepdim=True)\n",
    "dbnbias = dhpreact.sum(dim=0, keepdim=True)\n",
    "dbnvar = (dhpreact * (-0.5*bngain * bndiff * bnvar**-1.5)).sum(dim=0, keepdim=True)\n",
    "dbnmean = (dhpreact * -bngain * bnvar**-0.5).sum(dim=0, keepdim=True)\n",
    "temp1 = dhpreact * bngain * bnvar**-0.5\n",
    "temp2 = dbnmean * 1/batch_size * torch.ones_like(hprebn)\n",
    "temp3 = dbnvar * 2/(batch_size-1) * bndiff\n",
    "dhprebn = temp1 + temp2 + temp3\n",
    "# hidden layers\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(dim=0, keepdim=True)\n",
    "# input layer\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        k = Xb[i,j]\n",
    "        dC[k] += demb[i, j]\n",
    "\n",
    "\n",
    "cmp('dlogits', dlogits, logits)\n",
    "cmp('dh', dh, h)\n",
    "cmp('dW2', dW2, W2)\n",
    "cmp('db2', db2, b2)\n",
    "cmp('dhpreact', dhpreact, hpreact)\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbnbias', dbnbias, bnbias)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "cmp('demb', demb, emb)\n",
    "cmp('dC', dC, C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
