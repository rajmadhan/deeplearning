{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "533\n",
      "b'\\xef\\xbc\\xb5\\xef\\xbd\\x8e\\xef\\xbd\\x89\\xef\\xbd\\x83\\xef\\xbd\\x8f\\xef\\xbd\\x84\\xef\\xbd\\x85! \\xf0\\x9f\\x85\\xa4\\xf0\\x9f\\x85\\x9d\\xf0\\x9f\\x85\\x98\\xf0\\x9f\\x85\\x92\\xf0\\x9f\\x85\\x9e\\xf0\\x9f\\x85\\x93\\xf0\\x9f\\x85\\x94\\xe2\\x80\\xbd \\xf0\\x9f\\x87\\xba\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb3\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xae\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa8\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb4\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa9\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xaa! \\xf0\\x9f\\x98\\x84 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to \\xe2\\x80\\x9csupport Unicode\\xe2\\x80\\x9d in our software (whatever that means\\xe2\\x80\\x94like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don\\xe2\\x80\\x99t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode\\xe2\\x80\\x99s inception.'\n",
      "616\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(len(text))\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "616\n"
     ]
    }
   ],
   "source": [
    "tokens = list(map(int, tokens))\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge_tokens(ids, pair, new_token):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids)-1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "curr_tokens = tokens\n",
    "new_token = 256\n",
    "done = False\n",
    "merges = {}\n",
    "while True:\n",
    "    token_pair_cnt = get_counts(curr_tokens)\n",
    "    top_pair = max(token_pair_cnt, key=token_pair_cnt.get)\n",
    "    if token_pair_cnt[top_pair] == 1: break\n",
    "    merges[top_pair] = new_token\n",
    "    curr_tokens = merge_tokens(curr_tokens, top_pair, new_token)\n",
    "    new_token += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 266, 142, 266, 137, 266, 131, 266, 143, 266, 132, 266, 133, 296, 263, 164, 263, 157, 263, 152, 263, 146, 263, 158, 263, 147, 263, 148, 258, 189, 32, 264, 186, 268, 179, 268, 174, 268, 168, 268, 180, 268, 169, 268, 170, 296, 257, 152, 132, 32, 84, 104, 256, 297, 298, 110, 299, 286, 300, 301, 102, 287, 32, 288, 97, 302, 304, 104, 287, 305, 306, 314, 119, 270, 108, 100, 119, 315, 101, 291, 87, 316, 317, 107, 318, 119, 32, 302, 279, 319, 271, 116, 111, 32, 258, 156, 322, 270, 323, 324, 157, 32, 259, 32, 279, 114, 32, 115, 290, 116, 119, 265, 256, 40, 325, 326, 101, 297, 275, 97, 271, 109, 101, 261, 115, 258, 148, 108, 300, 256, 327, 293, 119, 99, 104, 265, 95, 271, 329, 97, 292, 278, 286, 272, 330, 105, 319, 116, 63, 41, 291, 66, 117, 323, 256, 332, 316, 98, 286, 327, 101, 276, 288, 100, 105, 118, 293, 304, 262, 279, 115, 274, 45, 112, 97, 103, 256, 284, 256, 83, 116, 274, 265, 100, 32, 112, 108, 117, 260, 105, 305, 333, 122, 295, 260, 306, 322, 108, 101, 109, 295, 116, 265, 298, 261, 110, 101, 120, 101, 330, 334, 270, 116, 285, 288, 318, 116, 301, 332, 256, 109, 270, 335, 294, 97, 32, 108, 105, 116, 116, 108, 256, 289, 105, 109, 315, 326, 272, 291, 73, 32, 333, 110, 258, 153, 271, 98, 108, 299, 314, 329, 273, 105, 317, 102, 259, 100, 272, 278, 325, 111, 108, 335, 293, 109, 121, 273, 269, 105, 279, 285, 101, 118, 295, 32, 51, 48, 32, 121, 287, 260, 97, 102, 116, 269, 32, 284, 324, 153, 260, 259, 99, 334, 116, 105, 111, 110, 46]\n",
      "300\n",
      "335\n"
     ]
    }
   ],
   "source": [
    "print(curr_tokens)\n",
    "print(len(curr_tokens))\n",
    "print(max(curr_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "533\n"
     ]
    }
   ],
   "source": [
    "def decode(ids, merges):\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    for (p0, p1), i in merges.items():\n",
    "        vocab[i] = vocab[p0] + vocab[p1]\n",
    "    tokens = b\"\".join(vocab[i] for i in ids)\n",
    "    text = tokens.decode('utf-8', errors='replace')\n",
    "    return text\n",
    "\n",
    "    tokens = []\n",
    "    cnt = 0\n",
    "    stack = []\n",
    "    for id in ids:\n",
    "        stack.append(id)\n",
    "        while len(stack):\n",
    "            if stack[-1] in merges:\n",
    "                pair = merges[stack[-1]]\n",
    "                stack[-1] = pair[1]\n",
    "                stack.append(pair[0])\n",
    "            else:\n",
    "                tokens.append(stack[-1])\n",
    "                del stack[-1]\n",
    "    return tokens\n",
    "\n",
    "decoded_tokens = decode(curr_tokens, merges)\n",
    "print(decoded_tokens)\n",
    "print(len(decoded_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 266, 142, 266, 137, 266, 131, 266, 143, 266, 132, 266, 133, 296, 263, 164, 263, 157, 263, 152, 263, 146, 263, 158, 263, 147, 263, 148, 258, 189, 32, 264, 186, 268, 179, 268, 174, 268, 168, 268, 180, 268, 169, 268, 170, 296, 257, 152, 132, 32, 84, 104, 256, 297, 298, 110, 299, 286, 300, 301, 102, 287, 32, 288, 97, 302, 304, 104, 287, 305, 306, 314, 119, 270, 108, 100, 119, 315, 101, 291, 87, 316, 317, 107, 318, 119, 32, 302, 279, 319, 271, 116, 111, 32, 258, 156, 322, 270, 323, 324, 157, 32, 259, 32, 279, 114, 32, 115, 290, 116, 119, 265, 256, 40, 325, 326, 101, 297, 275, 97, 271, 109, 101, 261, 115, 258, 148, 108, 300, 256, 327, 293, 119, 99, 104, 265, 95, 271, 329, 97, 292, 278, 286, 272, 330, 105, 319, 116, 63, 41, 291, 66, 117, 323, 256, 332, 316, 98, 286, 327, 101, 276, 288, 100, 105, 118, 293, 304, 262, 279, 115, 274, 45, 112, 97, 103, 256, 284, 256, 83, 116, 274, 265, 100, 32, 112, 108, 117, 260, 105, 305, 333, 122, 295, 260, 306, 322, 108, 101, 109, 295, 116, 265, 298, 261, 110, 101, 120, 101, 330, 334, 270, 116, 285, 288, 318, 116, 301, 332, 256, 109, 270, 335, 294, 97, 32, 108, 105, 116, 116, 108, 256, 289, 105, 109, 315, 326, 272, 291, 73, 32, 333, 110, 258, 153, 271, 98, 108, 299, 314, 329, 273, 105, 317, 102, 259, 100, 272, 278, 325, 111, 108, 335, 293, 109, 121, 273, 269, 105, 279, 285, 101, 118, 295, 32, 51, 48, 32, 121, 287, 260, 97, 102, 116, 269, 32, 284, 324, 153, 260, 259, 99, 334, 116, 105, 111, 110, 46]\n",
      "300\n",
      "335\n"
     ]
    }
   ],
   "source": [
    "def encode(text, merges):\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while True:\n",
    "        counts = get_counts(tokens)\n",
    "        pair = min(counts, key=lambda p: merges.get(p, float('inf')))\n",
    "        if pair not in merges: break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge_tokens(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "encoded_tokens = encode(text, merges)\n",
    "print(encoded_tokens)\n",
    "print(len(encoded_tokens))\n",
    "print(max(encoded_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(text == decode(encode(text, merges), merges))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
